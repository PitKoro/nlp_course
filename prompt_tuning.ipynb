{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petr/Projects/NLP/prompt_tuning/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import transformers\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thouroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    }
   ],
   "source": [
    "model_name = \"Enoch/llama-7b-hf\"\n",
    "tokenizer = transformers.LlamaTokenizer.from_pretrained(model_name, device_map=device)\n",
    "tokenizer.pad_token_id = tokenizer.eos_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petr/Projects/NLP/prompt_tuning/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "/home/petr/Projects/NLP/prompt_tuning/.venv/lib/python3.12/site-packages/transformers/utils/generic.py:311: FutureWarning: `torch.utils._pytree._register_pytree_node` is deprecated. Please use `torch.utils._pytree.register_pytree_node` instead.\n",
      "  torch.utils._pytree._register_pytree_node(\n",
      "Loading checkpoint shards: 100%|██████████| 33/33 [00:10<00:00,  3.19it/s]\n"
     ]
    }
   ],
   "source": [
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map=\"auto\",\n",
    "    low_cpu_mem_usage=True, #загружает веса по кусочкам, минимизируя потребление RAM при старте\n",
    "    offload_state_dict=True,# Если не хватит GPU, то часть модели перенесет на CPU\n",
    "    load_in_4bit=True, #Квантование до 4 бит с помощью bitsandbytes\n",
    "    torch_dtype=torch.float32 # нормировка по слоям и активации\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# замораживаем веса модели\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "позволяет экономить память\n",
    "почти не сохраняет активации в памяти, а пересчитывает их заново\n",
    "\"\"\"\n",
    "model.gradient_checkpointing_enable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.enable_input_require_grads()# отключаем градиенты для входных данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"A quick brown fox\"\n",
    "batch = tokenizer(prompt, return_tensors='pt', return_token_type_ids=False).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1,   319,  4996, 17354,  1701, 29916]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Параметры генерации\n",
    "# max_length = 100         # Максимальная длина генерируемой последовательности\n",
    "# num_beams = 5            # Количество \"лучей\" (beam search)\n",
    "# early_stopping = True    # Остановка, если все лучи достигли EOS\n",
    "# temperature=10\n",
    "# # Генерация с использованием beam search\n",
    "# outputs = model.generate(\n",
    "#     temperature=temperature,\n",
    "#     input_ids=batch['input_ids'],\n",
    "#     attention_mask=batch['attention_mask'],\n",
    "#     max_length=max_length,\n",
    "#     num_beams=num_beams,\n",
    "#     early_stopping=early_stopping,\n",
    "#     eos_token_id=tokenizer.eos_token_id  # Указываем токен конца последовательности\n",
    "# )\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Декодирование и вывод результата\n",
    "# print(tokenizer.decode(outputs[0], skip_special_tokens=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>A quick brown fox jumps over the lazy dog.\\nA quick'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# while batch['input_ids'][0].cpu().tolist()[-1] != tokenizer.eos_token_id:\n",
    "for _ in range(10):\n",
    "    logits = model(**batch).logits\n",
    "    last_state_for_first_batch = logits[0, -1]\n",
    "    greedy_token = last_state_for_first_batch.argmax(-1)\n",
    "    new_token = greedy_token.reshape(1,1)\n",
    "    batch['input_ids'] = torch.cat([batch['input_ids'], new_token], dim=-1)\n",
    "    batch['attention_mask'] = torch.cat([batch['attention_mask'], torch.ones_like(new_token)], dim=-1)\n",
    "\n",
    "\n",
    "tokenizer.decode(batch['input_ids'][0].cpu().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn import functional as F\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(2.8630, device='cuda:0', grad_fn=<NllLossBackward0>)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "the_truth = \"A quick brown fox jumps over the lazy dog. Besides, that dog deserved it anyway!\"\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "output = model(**batch)\n",
    "\n",
    "next_word_logits = output.logits[:,:-1,:]\n",
    "true_next_tokens = batch['input_ids'][:,1:]\n",
    "loss = F.cross_entropy(next_word_logits.flatten(0,1), true_next_tokens.flatten(0,1))\n",
    "\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WordEmbeddingsWithLearnedPrompts(nn.Module):\n",
    "    def __init__(self, word_embeddings: nn.Embedding, num_prompts: int):\n",
    "        super().__init__()\n",
    "        self.original_word_embeddings = word_embeddings\n",
    "        self.num_prompts = num_prompts\n",
    "        self.learneble_prompts = nn.Parameter(\n",
    "            data=torch.randn(1, self.num_prompts, self.original_word_embeddings.embedding_dim),\n",
    "            requires_grad=True\n",
    "        )\n",
    "        \n",
    "    def forward(self, input_ids):\n",
    "        assert input_ids.dtype == torch.int64\n",
    "        assert input_ids.shape[1] > self.num_prompts \n",
    "        assert torch.all(input_ids[:, :self.num_prompts] == tokenizer.pad_token_id).item(), \"Не забудьте добавть паддинги в начало последовательности для подстановки обучаемых параметров\"\n",
    "        \n",
    "        original_embeddings = self.original_word_embeddings(input_ids)# получилось что [1, num_prompts + num_input_tokens, emb_dim]\n",
    "        embedded_inputs_with_prompts = torch.cat(\n",
    "            [self.learneble_prompts.expand(input_ids.shape[0], -1, -1),\n",
    "            original_embeddings[:,self.num_prompts:]],\n",
    "            dim=1\n",
    "        )\n",
    "        \n",
    "        return embedded_inputs_with_prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looks legit!\n"
     ]
    }
   ],
   "source": [
    "num_prompts = 16\n",
    "test_emb_layer = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "test_input_ids = tokenizer(\"a cat says on a may\", return_tensors='pt')[\"input_ids\"].to(device)\n",
    "\n",
    "space_for_prompts = torch.full(\n",
    "    [\n",
    "        len(test_input_ids), num_prompts\n",
    "    ],\n",
    "    fill_value=tokenizer.pad_token_id,\n",
    "    dtype=torch.int64,\n",
    "    device=device\n",
    ")\n",
    "test_inputs_with_prompts = torch.cat([space_for_prompts, test_input_ids],dim=1)\n",
    "with torch.amp.autocast('cuda'):\n",
    "    test_prompt_embeddings = test_emb_layer(test_inputs_with_prompts)\n",
    "\n",
    "assert test_prompt_embeddings.shape[:2] == test_inputs_with_prompts.shape\n",
    "assert test_prompt_embeddings.shape[-1] == model.config.hidden_size\n",
    "assert torch.allclose(test_prompt_embeddings[:,:num_prompts], test_emb_layer.learneble_prompts.float())\n",
    "assert torch.allclose(test_prompt_embeddings[:,num_prompts:], model.model.embed_tokens(test_input_ids).float())\n",
    "print(\"Looks legit!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(model.model.embed_tokens, nn.Embedding), \"Вы уже заменили Embedding слой\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.model.embed_tokens = WordEmbeddingsWithLearnedPrompts(model.model.embed_tokens, num_prompts=num_prompts).to(device)\n",
    "\n",
    "opt = torch.optim.Adam([model.model.embed_tokens.learneble_prompts], lr=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0, Loss: 7.224433422088623\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/petr/Projects/NLP/prompt_tuning/.venv/lib/python3.12/site-packages/torch/_dynamo/eval_frame.py:745: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. In version 2.5 we will raise an exception if use_reentrant is not passed. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
      "  return fn(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 6.526238918304443\n",
      "Epoch: 2, Loss: 6.100164890289307\n",
      "Epoch: 3, Loss: 5.700748920440674\n",
      "Epoch: 4, Loss: 5.346858501434326\n",
      "Epoch: 5, Loss: 5.039243698120117\n",
      "Epoch: 6, Loss: 4.760209560394287\n",
      "Epoch: 7, Loss: 4.498955726623535\n",
      "Epoch: 8, Loss: 4.2563276290893555\n",
      "Epoch: 9, Loss: 4.033092975616455\n",
      "Epoch: 10, Loss: 3.8236215114593506\n",
      "Epoch: 11, Loss: 3.620473623275757\n",
      "Epoch: 12, Loss: 3.4202325344085693\n",
      "Epoch: 13, Loss: 3.2247185707092285\n",
      "Epoch: 14, Loss: 3.0371389389038086\n",
      "Epoch: 15, Loss: 2.85799241065979\n",
      "Epoch: 16, Loss: 2.685307741165161\n",
      "Epoch: 17, Loss: 2.5175652503967285\n",
      "Epoch: 18, Loss: 2.355043888092041\n",
      "Epoch: 19, Loss: 2.1985714435577393\n",
      "Epoch: 20, Loss: 2.0475263595581055\n",
      "Epoch: 21, Loss: 1.900288462638855\n",
      "Epoch: 22, Loss: 1.757215976715088\n",
      "Epoch: 23, Loss: 1.6206257343292236\n",
      "Epoch: 24, Loss: 1.4905178546905518\n",
      "Epoch: 25, Loss: 1.3647634983062744\n",
      "Epoch: 26, Loss: 1.2436720132827759\n",
      "Epoch: 27, Loss: 1.1281546354293823\n",
      "Epoch: 28, Loss: 1.0155348777770996\n",
      "Epoch: 29, Loss: 0.9033856987953186\n",
      "Epoch: 30, Loss: 0.7936357259750366\n",
      "Epoch: 31, Loss: 0.6894252896308899\n",
      "Epoch: 32, Loss: 0.5930396914482117\n",
      "Epoch: 33, Loss: 0.5066859722137451\n",
      "Epoch: 34, Loss: 0.43000566959381104\n",
      "Epoch: 35, Loss: 0.3606102764606476\n",
      "Epoch: 36, Loss: 0.2995564937591553\n",
      "Epoch: 37, Loss: 0.2495162934064865\n",
      "Epoch: 38, Loss: 0.21037721633911133\n",
      "Epoch: 39, Loss: 0.1802590936422348\n",
      "Epoch: 40, Loss: 0.15696658194065094\n",
      "Epoch: 41, Loss: 0.13804151117801666\n",
      "Epoch: 42, Loss: 0.12157619744539261\n",
      "Epoch: 43, Loss: 0.10702360421419144\n",
      "Epoch: 44, Loss: 0.09453464299440384\n",
      "Epoch: 45, Loss: 0.0839662030339241\n",
      "Epoch: 46, Loss: 0.07486452162265778\n",
      "Epoch: 47, Loss: 0.06687642633914948\n",
      "Epoch: 48, Loss: 0.059836652129888535\n",
      "Epoch: 49, Loss: 0.053632304072380066\n",
      "Epoch: 50, Loss: 0.04815927520394325\n",
      "Epoch: 51, Loss: 0.04335144907236099\n",
      "Epoch: 52, Loss: 0.03916832432150841\n",
      "Epoch: 53, Loss: 0.03555702790617943\n",
      "Epoch: 54, Loss: 0.03244238346815109\n",
      "Epoch: 55, Loss: 0.02974770776927471\n",
      "Epoch: 56, Loss: 0.027411190792918205\n",
      "Epoch: 57, Loss: 0.025387579575181007\n",
      "Epoch: 58, Loss: 0.02363823726773262\n",
      "Epoch: 59, Loss: 0.022123221307992935\n",
      "Epoch: 60, Loss: 0.020801274105906487\n",
      "Epoch: 61, Loss: 0.01963391713798046\n",
      "Epoch: 62, Loss: 0.018589291721582413\n",
      "Epoch: 63, Loss: 0.01764506846666336\n",
      "Epoch: 64, Loss: 0.016786694526672363\n",
      "Epoch: 65, Loss: 0.016005175188183784\n",
      "Epoch: 66, Loss: 0.015293631702661514\n",
      "Epoch: 67, Loss: 0.014645443297922611\n",
      "Epoch: 68, Loss: 0.014053412713110447\n",
      "Epoch: 69, Loss: 0.013510181568562984\n",
      "Epoch: 70, Loss: 0.01300855539739132\n",
      "Epoch: 71, Loss: 0.012542743235826492\n",
      "Epoch: 72, Loss: 0.012108206748962402\n",
      "Epoch: 73, Loss: 0.011701812967658043\n",
      "Epoch: 74, Loss: 0.011321461759507656\n",
      "Epoch: 75, Loss: 0.01096540316939354\n",
      "Epoch: 76, Loss: 0.010632235556840897\n",
      "Epoch: 77, Loss: 0.010320440866053104\n",
      "Epoch: 78, Loss: 0.010028142482042313\n",
      "Epoch: 79, Loss: 0.009753811173141003\n",
      "Epoch: 80, Loss: 0.009495803155004978\n",
      "Epoch: 81, Loss: 0.00925245601683855\n",
      "Epoch: 82, Loss: 0.009022765792906284\n",
      "Epoch: 83, Loss: 0.008805632591247559\n",
      "Epoch: 84, Loss: 0.00860039982944727\n",
      "Epoch: 85, Loss: 0.008406260050833225\n",
      "Epoch: 86, Loss: 0.00822268147021532\n",
      "Epoch: 87, Loss: 0.00804906990379095\n",
      "Epoch: 88, Loss: 0.00788450613617897\n",
      "Epoch: 89, Loss: 0.0077284593135118484\n",
      "Epoch: 90, Loss: 0.007580020930618048\n",
      "Epoch: 91, Loss: 0.007438539993017912\n",
      "Epoch: 92, Loss: 0.007303346414119005\n",
      "Epoch: 93, Loss: 0.0071738543920218945\n",
      "Epoch: 94, Loss: 0.007049615494906902\n",
      "Epoch: 95, Loss: 0.006930310744792223\n",
      "Epoch: 96, Loss: 0.006815494503825903\n",
      "Epoch: 97, Loss: 0.006705098319798708\n",
      "Epoch: 98, Loss: 0.00659863231703639\n",
      "Epoch: 99, Loss: 0.006495990324765444\n",
      "Good job!\n"
     ]
    }
   ],
   "source": [
    "the_truth = [\"A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway!\"]\n",
    "batch = tokenizer(the_truth, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "\n",
    "\n",
    "space_for_prompts = torch.full(\n",
    "    size=[\n",
    "        batch['input_ids'].size()[0],\n",
    "        num_prompts\n",
    "    ],\n",
    "    fill_value=tokenizer.eos_token_id,\n",
    "    dtype=torch.int64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "input_ids_with_padding = torch.cat(\n",
    "    [\n",
    "        space_for_prompts,\n",
    "        batch['input_ids']\n",
    "    ],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "batch['input_ids'] = input_ids_with_padding\n",
    "batch['attention_mask'] = torch.cat(\n",
    "    [\n",
    "        torch.ones_like(space_for_prompts),\n",
    "        batch['attention_mask']\n",
    "    ],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "\n",
    "EPOCHS = 100\n",
    "model.train()\n",
    "for epoch in range(EPOCHS):\n",
    "    output = model(**batch)\n",
    "    next_word_logits = output.logits[:, num_prompts : -1, :]\n",
    "    true_next_token = batch['input_ids'][:,num_prompts + 1:]\n",
    "    loss = F.cross_entropy(next_word_logits.flatten(0,1), true_next_token.flatten(0,1))\n",
    "    print(f\"Epoch: {epoch}, Loss: {loss}\")\n",
    "    opt.zero_grad()\n",
    "    loss.backward()\n",
    "    opt.step()\n",
    "    \n",
    "    \n",
    "assert loss.item() <= 0.1\n",
    "print(\"Good job!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): WordEmbeddingsWithLearnedPrompts(\n",
       "      (original_word_embeddings): Embedding(32000, 4096, padding_idx=0)\n",
       "    )\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLUActivation()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm()\n",
       "        (post_attention_layernorm): LlamaRMSNorm()\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 6]),\n",
       " {'input_ids': tensor([[    1,   319,  4996, 17354,  1701, 29916]], device='cuda:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1]], device='cuda:0')})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"A quick brown fox\"\n",
    "batch = tokenizer(text, return_tensors='pt', return_token_type_ids=False).to(device)\n",
    "batch['input_ids'].size(), batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "space_for_prompts = torch.full(\n",
    "    size=[\n",
    "        batch['input_ids'].size()[0],\n",
    "        num_prompts\n",
    "    ],\n",
    "    fill_value=tokenizer.eos_token_id,\n",
    "    dtype=torch.int64,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "input_ids_with_padding = torch.cat(\n",
    "    [\n",
    "        space_for_prompts,\n",
    "        batch['input_ids']\n",
    "    ],\n",
    "    dim=1\n",
    ")\n",
    "\n",
    "batch['input_ids'] = input_ids_with_padding\n",
    "batch['attention_mask'] = torch.cat(\n",
    "    [\n",
    "        torch.ones_like(space_for_prompts),\n",
    "        batch['attention_mask']\n",
    "    ],\n",
    "    dim=1\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 22]), torch.Size([1, 22]))"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch['input_ids'].size(), batch['attention_mask'].size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for _ in range(30):\n",
    "    output = model(**batch)\n",
    "    logits = output.logits\n",
    "    next_token = output.logits[0][-1].argmax(-1)\n",
    "    batch[\"input_ids\"] = torch.cat([batch[\"input_ids\"], next_token.reshape(1,1)], dim=1)\n",
    "    batch[\"attention_mask\"] = torch.cat([batch[\"attention_mask\"], torch.tensor(1, device=device).reshape(1,1)], dim=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "</s></s></s></s></s></s></s></s></s></s></s></s></s></s></s></s><s>A quick brown fox did not jump over the lazy dog. Besides, that dog deserved it anyway! Besides that dog deserved it anyway! That dog deserved it "
     ]
    }
   ],
   "source": [
    "print(tokenizer.decode(batch[\"input_ids\"][0]), end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
